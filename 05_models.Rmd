---
title: "Models"
output: 
      html_document:
          toc: true
          toc_depth: 2
          toc_float: true
---


```{r echo=FALSE}
knitr::opts_chunk$set(cache = T)
```

<style>
body {
text-align: justify}
</style>

<br>

I'm learning Maximum Likelihood Estimation...

# overview 


# Descriptives, distribution of data



# Basic Model 

## AGB ~ a·TCH^b^



## AGB ~ a·TCH^b^·BA^c^



## AGB ~ a·TCH^b^·BA^c^·ρ^d^





# David's comment:

$$ACD \sim  a· TCH^b$$

Nls or  LSR of log-log transformed data. 
Making a and b a functional of environmental information (e.g TPI — topographic position index)  


$$ACD \sim  a · wood density · TCH^b  + e $$

$$e \sim N(0,sigma + sigma1·TCH)$$

Coomes wind in New Zealand —  Maximum Likelihood Estimation — log likelihood 
```{r eval=FALSE}
start = list(a = .3,  b = 1,  sigma = 0.01, sigma1 = 0.001)
```

Multiply likelihoods === summing log-likelihood —- algorithm maximizes the log likelihood.

A, b and wood density vary with environmental variables.  Cristiano or/and  Eric’s many layers.

$$ACD \sim  a·(1+a1·TPI + a2·rainfall) · Wood\ Density · TCH^b·(1+ b1·TPI + b2·rainfall)  + e$$ 
$$e ~ N(0,sigma + sigma1·TCH)$$

<br>



# Edgar's first attempt

I have some questions: 
- how to define the error structure? 
- What is the proper way to assign "priors"?
- Which other MLE methods are appropriate?
- etc...

```{r eval=FALSE}
# load libraries
library(bbmle)

#functions
RMSE<-function(Obs,Pred){
  sqrt(mean((Obs-Pred)^2) ) / mean(Obs) *100 }
bias<-function(Obs,Pred){
  mean(Pred-Obs)/mean(Obs)*100 }
R2 <-function(Obs,Pred){ 1 - sum((Obs-Pred)^2) / sum((Obs-mean(Obs))^2) }

XXX <- read.csv('~/Rproj/07_carbon_br/data/to_model/20210420_db_model_v1.csv')
XXX$ACD.y <- XXX$totalbiomas.wplog2.area*10000*.47


Y <- XXX[, c("ACD.y", "TCHmean")]
names(Y)

# quick look at correlations

summary(lm(log(ACD.y) ~ log(TCHmean), data = Y))
summary(lm(ACD.y ~ TCHmean, data = Y))

plot(Y$ACD.y ~ Y$TCHmean)


#################################
## model of TCH influence on AGB

n = nrow(Y)
Coeffs = data.frame(pred = rep(NA,n), obs = rep(NA,n), a = rep(NA,n),b = rep(NA,n), sigma = rep(NA,n), sigma1 = rep(NA,n) )

for (i in 1:nrow(Y)) {
  # this is leave-one-out regression, where you fit to all data but one and then predict that point to get R2 of observed vs predicted
  
  ## this log-likehihood function takes a bit to get your head round.
  LL <- function(a, b,sigma) {
    with(Y[-i,],
         {
           R = ACD.y - a*TCHmean^b
           R = suppressWarnings(dnorm(R, 0, sigma, log = TRUE))
           -sum(R) })}
  # this uses maximum likelihood estimation to find the parameters that minimise log likehood = maximise likelihood
  fit1 <- mle2(LL, start = list(a = 1, b =1, sigma =.1), method = "L-BFGS-B",
               lower = c(-Inf, -Inf, 0.0000001), hessian =TRUE,
               upper = c(Inf, Inf, Inf))
  Coeffs$obs[i] = Y$ACD.y[i]
  Coeffs$pred[i] = Y$TCHmean[i]*fit1@coef[1]^fit1@coef[2]
  Coeffs$a[i] = fit1@coef[1]
  Coeffs$b[i] = fit1@coef[2]
  Coeffs$sigma[i] = fit1@coef[3]
}

summary(fit1)
plot(Coeffs$obs, Coeffs$pred)
abline(a=1, b=0)
RMSE(Coeffs$obs, Coeffs$pred)
bias(Coeffs$obs, Coeffs$pred)
R2(Coeffs$obs, Coeffs$pred)


```

<br>

# David/Sacha MLE reference code

Here is the code for maximum likelihood estimation from David/Sacha:

```{r eval=FALSE}
# David Coomes August 2021, analyses of Sacha's data

b*Canopy_area^a*(1+ c*Pp )*(1+d*SRe)
b*Canopy_area^a*(1+ c*Pp*(1+c1*Pp) )*(1+d*SRe))
b*Canopy_area^a*(1+ c*Ppn +c1*Pps + c2*Pqf)*(1+d*SRe)

# load libraries
library(bbmle)

#functions
RMSE<-function(Obs,Pred){
  sqrt(mean((Obs-Pred)^2) ) / mean(Obs) *100 }
bias<-function(Obs,Pred){
  mean(Pred-Obs)/mean(Obs)*100 }
R2 <-function(Obs,Pred){ 1 - sum((Obs-Pred)^2) / sum((Obs-mean(Obs))^2) }

# amalgamate data dispersed over three files
setwd("C://Users//david coomes//Downloads")
XXX <- read.table("modispixeldata.CSV", sep =",", header =T)
XXX <- read.table("fundivplotdata.CSV", sep =",", header =T)
names(XXX)

Y <- XXX[, c("AWPmean.y", "Ppn", "Pps","Pqf","Pqi", "oakpine", "Pp", "Canopy_area", "AWPsens")]

Y$SRe = with(Y, exp(-(Ppn*log(Ppn) + Pps*log(Pps)+Pqf*log(Pqf)+Pqi*log(Pqi))))
summary(Y$SRe)

names(Y)

# quick look at correlations
summary(lm(AWPmean.y ~ oakpine+Canopy_area + shannoncanopy, data = Y))
summary(lm(log(AWPmean.y) ~ log(Canopy_area), data = Y))
summary(lm(AWPmean.y ~ Canopy_area, data = Y))

summary(lm(Canopy_area ~Pp +SRe, data = Y))
summary(lm(Canopy_area ~SRe, data = Y))
summary(lm(Canopy_area ~Pp, data = Y))

summary(lm(Pp~ SRe, data = Y))

plot(Y$AWPmean.y, Y$Canopy_area)
plot(Y$Canopy_area,Y$Pp)


#################################
## model of CA, Pp and SRe influence on AWP

n = nrow(Y)
Coeffs = data.frame(pred = rep(NA,n), obs = rep(NA,n), a = rep(NA,n),b = rep(NA,n), c = rep(NA,n), c1 = rep(NA,n), c2 = rep(NA,n), d = rep(NA,n), g = rep(NA,n), sigma = rep(NA,n), sigma1 = rep(NA,n) )

for (i in 1:nrow(Y)) {
  # this is leave-one-out regression, where you fit to all data but one and then predict that point to get R2 of observed vs predicted
  
  ## this log-likehihood function takes a bit to get your head round.
  LL <- function(a, b, c, d,sigma) {
    with(Y[-i,],
         {
           R = AWPmean.y - (b*Canopy_area^a)*(1+ c*Pp )*(1+d*SRe)
           R = suppressWarnings(dnorm(R, 0, sigma, log = TRUE))
           -sum(R) })}
  # this uses maximum likelihood estimation to find the parameters that minimise log likehood = maximise likelihood
  fit1 <- mle2(LL, start = list(a = 1, b =0.00038, c =0.1, d= 0.01, sigma =.17), method = "L-BFGS-B",
               lower = c(-Inf, -Inf, -Inf, -Inf, 0.0000001), hessian =TRUE,
               upper = c(Inf, Inf, Inf, Inf, Inf))
  Coeffs$obs[i] = Y$AWPmean.y[i]
  Coeffs$pred[i] = (Y$Canopy_area[i]*fit1@coef[2]^fit1@coef[1])*(1 + fit1@coef[3]*Y$Pp[i])*(1 + fit1@coef[3]*Y$Pp[i])*(1+fit1@coef[4]*(Y$SRe))
  Coeffs$a[i] = fit1@coef[1]
  Coeffs$b[i] = fit1@coef[2]
  Coeffs$c[i] = fit1@coef[3]
  Coeffs$d[i] = fit1@coef[4]
  Coeffs$sigma[i] = fit1@coef[5]
}

summary(fit1)

```


